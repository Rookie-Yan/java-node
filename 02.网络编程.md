# 1. 基础知识

## 1. 同步异步、阻塞非阻塞

同步：在发起请求之后，继续负责后续的返回结果的主动获取

异步：在发起请求之后，处理其他事务，后续返回结果的获取由其他线程被动通知

阻塞：在一个请求或任务没有结束之前，不允许处理其他事务，尽管这个任务需要等待

非阻塞：在一个请求或任务没有结束时，如果进行等待，则允许执行其他任务

多路复用：其实还是同步的、非阻塞的，使用select对单线程多个channel的事件进行监听，如果有事件就进行调度处理，没有事件就阻塞

## 2. NIO与BIO

BIO（阻塞IO）：一个连接一个线程，客户端有连接请求时服务器端就需要启动一个线程进行处理。线程开销大。

NIO（非阻塞IO）：一个请求一个线程，但客户端发送的连接请求都会注册到多路复用器上，多路复用器轮询到连接有I/O请求时才启动一个线程进行处理。

AIO（异步IO Asynchronous I/O）：一个有效请求一个线程，客户端的I/O请求都是由OS先完成了再通知服务器应用去启动线程进行处理。（事件都是由服务器先																处理，然后通知应用程序）

**💛BIO和BIO的比较**

* BIO是面向流的，NIO是面向缓冲区的
* BIO的各种流是阻塞的，而NIO是非阻塞的
* BIO的Stream是单向的，而NIO的channel是双向的



# 2. NIO

## 1. 模型与三要素

https://img-blog.csdnimg.cn/20190422121139668.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L20wXzM4MTA5MDQ2,size_16,color_FFFFFF,t_70

​		对于单个线程，都具有一个selector，负责对多个channel中的事件进行监控，当有事件发生时，就会让线程进行处理，没有时就进行阻塞。

​		而对于每个channel都有其对应的buffer，当数据进行读写操作时，先将数据写入buffer中，然后再读到JAVA内存中或写入系统内存中

* 三要素：selector（选择器）、channel（通道）、buffer（缓冲区）

* buffer：最常用的时bytebuffer，其中又有主要的三个参数：position、limit和capacity

  * position：当前指针位置
  * limit：末尾指针位置
  * capacity：buffer容量

  **常用的方法：**

  * flip：因为buffer是双向的，所以写切换到读需要通过flip进行切换
  * clear：由读切换到写，并且将position置为0，进行写
  * compact：由读切换到写，但是将position切换到没有读过数据的后面一个
  * rewind：重新将position置为00，进行读操作
  * mark/reset：mark标记当前位置，reset则从标记位置重新进行读

## 2. NIO模型的建立

* Selector.open()：打开一个Selector
* ServerSocketChannel.open()：创建服务端的Channel
* bind()：绑定到某个端口上。并配置非阻塞模式
* register()：注册Channel和关注的事件到Selector上；
* select()轮询拿到已经就绪的事件

## 3. 如何确定bytebuffer长度大小？

​		如果太小，传输效率过低；如果太大，占用空间过大。

​		netty使用的方案是根据数据传输大小进行扩容和缩容。如果发现数据过大，会先将原有读到的数据进行存储，然后作为附件绑定到selector，之后将buffer进行扩容，下一次读的时候，就会将附件先放到buffer中，在之后进行读。

​		buffer初始化容量有两种方式：allocate(**int** capacity) 和 allocateDirect (**int** capacity)，前面是从java内存中划分，后面是从直接内存中划分

​		使用直接内存可以减少一次用户态和内核态的切换。

## 4. 粘包与拆包

**造成的原因：**

1. 要发送的数据大于TCP发送缓冲区剩余空间大小，将会发生拆包。
2. 待发送数据大于MSS（最大报文长度），TCP在传输前将进行拆包。
3. 要发送的数据小于TCP发送缓冲区的大小，TCP将多次写入缓冲区的数据一次发送出去，将会发生粘包。
4. 接收数据端的应用层没有及时读取接收缓冲区中的数据，将发生粘包。

**解决方案：**

1、发送端给每个数据包添加包首部，首部中应该至少包含数据包的长度，这样接收端在接收到数据后，通过读取包首部的长度字段，便知道每一个数据包的实际长度了。

2、发送端将每个数据包封装为固定长度（不够的可以通过补0填充），这样接收端每次从接收缓冲区中读取固定长度的数据就自然而然的把每个数据包拆分开来。

3、可以在数据包之间设置边界，如添加特殊符号，接收端通过这个边界就可以将不同的数据包拆分开。

## 5. 零拷贝

### 5.1 传统模型

内部工作流程是这样的：

![](E:/java工具/知识体系/待整理/Netty网络编程/Netty教程源码资料/讲义/Netty-讲义/img/0024.png)

1. java 本身并不具备 IO 读写能力，因此 read 方法调用后，要从 java 程序的**用户态**切换至**内核态**，去调用操作系统（Kernel）的读能力，将数据读入**内核缓冲区**。这期间用户线程阻塞，操作系统使用 DMA（Direct Memory Access）来实现文件读，其间也不会使用 cpu

   > DMA 也可以理解为硬件单元，用来解放 cpu 完成文件 IO

2. 从**内核态**切换回**用户态**，将数据从**内核缓冲区**读入**用户缓冲区**（即 byte[] buf），这期间 cpu 会参与拷贝，无法利用 DMA

3. 调用 write 方法，这时将数据从**用户缓冲区**（byte[] buf）写入 **socket 缓冲区**，cpu 会参与拷贝

4. 接下来要向网卡写数据，这项能力 java 又不具备，因此又得从**用户态**切换至**内核态**，调用操作系统的写能力，使用 DMA 将 **socket 缓冲区**的数据写入网卡，不会使用 cpu



可以看到中间环节较多，java 的 IO 实际不是物理设备级别的读写，而是缓存的复制，底层的真正读写是操作系统来完成的

* 用户态与内核态的切换发生了 3 次，这个操作比较重量级
* 数据拷贝了共 4 次

### 5.2 优化模型

通过 DirectByteBuf 

* ByteBuffer.allocate(10)  HeapByteBuffer 使用的还是 java 内存
* ByteBuffer.allocateDirect(10)  DirectByteBuffer 使用的是操作系统内存

![](E:/java工具/知识体系/待整理/Netty网络编程/Netty教程源码资料/讲义/Netty-讲义/img/0025.png)

大部分步骤与优化前相同，不再赘述。唯有一点：java 可以使用 DirectByteBuf 将堆外内存映射到 jvm 内存中来直接访问使用

* 这块内存不受 jvm 垃圾回收的影响，因此内存地址固定，有助于 IO 读写
* java 中的 DirectByteBuf 对象仅维护了此内存的虚引用，内存回收分成两步
  * DirectByteBuf 对象被垃圾回收，将虚引用加入引用队列
  * 通过专门线程访问引用队列，根据虚引用释放堆外内存
* 减少了一次数据拷贝，用户态与内核态的切换次数没有减少



进一步优化（底层采用了 linux 2.1 后提供的 sendFile 方法），java 中对应着两个 channel 调用 transferTo/transferFrom 方法拷贝数据

![](E:/java工具/知识体系/待整理/Netty网络编程/Netty教程源码资料/讲义/Netty-讲义/img/0026.png)

1. java 调用 transferTo 方法后，要从 java 程序的**用户态**切换至**内核态**，使用 DMA将数据读入**内核缓冲区**，不会使用 cpu
2. 数据从**内核缓冲区**传输到 **socket 缓冲区**，cpu 会参与拷贝
3. 最后使用 DMA 将 **socket 缓冲区**的数据写入网卡，不会使用 cpu

可以看到

* 只发生了一次用户态与内核态的切换
* 数据拷贝了 3 次



进一步优化（linux 2.4）

![](E:/java工具/知识体系/待整理/Netty网络编程/Netty教程源码资料/讲义/Netty-讲义/img/0027.png)

1. java 调用 transferTo 方法后，要从 java 程序的**用户态**切换至**内核态**，使用 DMA将数据读入**内核缓冲区**，不会使用 cpu
2. 只会将一些 offset 和 length 信息拷入 **socket 缓冲区**，几乎无消耗
3. 使用 DMA 将 **内核缓冲区**的数据写入网卡，不会使用 cpu

整个过程仅只发生了一次用户态与内核态的切换，数据拷贝了 2 次。所谓的【零拷贝】，并不是真正无拷贝，而是在不会拷贝重复数据到 jvm 内存中，零拷贝的优点有

* 更少的用户态与内核态的切换
* 不利用 cpu 计算，减少 cpu 缓存伪共享
* 零拷贝适合小文件传输



## 6. NIO的事件类型

* OP_ACCEPT：当收到一个客户端的连接请求时，该操作就绪。这是ServerSocketChannel上唯一有效的操作。
* OP_CONNECT：只有客户端SocketChannel会注册该操作，当客户端调用SocketChannel.connect()时，该操作会就绪。
* OP_READ：该操作对客户端和服务端的SocketChannel都有效，当OS的读缓冲区中有数据可读时，该操作就绪。当客户端正常或异常断开时，也会触发一次读事件，但是返回值为-1
* OP_WRITE：该操作对客户端和服务端的SocketChannel都有效，当OS的写缓冲区中有空闲的空间时(大部分时候都有)，该操作就绪。

# 3. netty

Netty是 一个**异步事件驱动**的网络应用程序框架，**用于快速开发可维护**的高性能协议服务器和客户端。Netty是**基于NIO**的，它封装了jdk的NIO，让我们使用起来更加方法灵活。

## 1. netty的特点

* 高并发：Netty 是一款基于 NIO（Nonblocking IO，非阻塞IO）开发的网络通信框架，对比于 BIO（Blocking I/O，阻塞IO），他的并发性能得到了很大提高。
* 传输快：Netty 的传输依赖于零拷贝特性，尽量减少不必要的内存拷贝，实现了更高效率的传输。
* 封装好：Netty 封装了 NIO 操作的很多细节，提供了易于使用调用接口。

## 2. netty的优势

* IO 线程模型：同步非阻塞，用最少的资源做更多的事。
* 内存零拷贝：尽量减少不必要的内存拷贝，实现了更高效率的传输。
* 内存池设计：申请的内存可以重用，主要指直接内存。内部实现是用一颗二叉查找树管理内存分配情况。
* 串形化处理读写：避免使用锁带来的性能开销。
* 高性能序列化协议：支持 protobuf 等高性能序列化协议



## 3. 线程模型

​		Netty通过Reactor模型基于多路复用器接收并处理用户请求，内部实现了两个线程池，boss线程池和work线程池，其中boss线程池的线程负责处理请求的accept事件，当接收到accept事件的请求时，把对应的socket封装到一个NioSocketChannel中，并交给work线程池，其中work线程池负责请求的read和write事件，由对应的Handler处理。

* 单线程模型：所有I/O操作都由一个线程完成，即多路复用、事件分发和处理都是在一个Reactor线程上完成的。既要接收客户端的连接请求,向服务端发起连接，又要发送/读取请求或应答/响应消息。一个NIO 线程同时处理成百上千的链路，性能上无法支撑，速度慢，若线程进入死循环，整个程序不可用，对于高负载、大并发的应用场景不合适。

* 多线程模型：有一个NIO 线程（Acceptor） 只负责监听服务端，接收客户端的TCP 连接请求；NIO 线程池负责网络IO 的操作，即消息的读取、解码、编码和发送；1 个NIO 线程可以同时处理N 条链路，但是1 个链路只对应1 个NIO 线程，这是为了防止发生并发操作问题。但在并发百万客户端连接或需要安全认证时，一个Acceptor 线程可能会存在性能不足问题。

* 主从多线程模型：Acceptor 线程用于绑定监听端口，接收客户端连接，将SocketChannel 从主线程池的Reactor 线程的多路复用器上移除，重新注册到Sub 线程池的线程上，用于处理I/O 的读写等操作，从而保证mainReactor只负责接入认证、握手等操作；



## 4. 主要组件

* Channel：Netty 网络操作抽象类，它除了包括基本的 I/O 操作，如 bind、connect、read、write 等。
* EventLoop：主要是配合 Channel 处理 I/O 操作，用来处理连接的生命周期中所发生的事情。
* ChannelFuture：Netty 框架中所有的 I/O 操作都为异步的，因此我们需要 ChannelFuture 的 addListener()注册一个 ChannelFutureListener 监听事件，当操作执行成功或者失败时，监听就会自动触发返回结果。
* ChannelHandler：充当了所有处理入站和出站数据的逻辑容器。ChannelHandler 主要用来处理各种事件，这里的事件很广泛，比如可以是连接、数据接收、异常、数据转换等。
* ChannelPipeline：为 ChannelHandler 链提供了容器，当 channel 创建时，就会被自动分配到它专属的 ChannelPipeline，这个关联是永久性的。



## 5. 消息发送方式

- 直接写入 Channel 中，消息从 ChannelPipeline 当中尾部开始移动；
- 写入和 ChannelHandler 绑定的 ChannelHandlerContext 中，消息从 ChannelPipeline 中的下一个 ChannelHandler 中移动。



## 6. 默认情况 Netty 起多少线程？何时启动？

Netty 默认是 CPU 处理器数的两倍，bind 完之后启动。



## 7. netty的零拷贝

Netty 的零拷贝主要包含三个方面：

* Netty 的接收和发送 ByteBuffer 采用 DIRECT BUFFERS，使用堆外直接内存进行 Socket 读写，不需要进行字节缓冲区的二次拷贝。如果使用传统的堆内存（HEAP BUFFERS）进行 Socket 读写，JVM 会将堆内存 Buffer 拷贝一份到直接内存中，然后才写入 Socket 中。相比于堆外直接内存，消息在发送过程中多了一次缓冲区的内存拷贝。
* Netty 提供了组合 Buffer 对象，可以聚合多个 ByteBuffer 对象，用户可以像操作一个 Buffer 那样方便的对组合 Buffer 进行操作，避免了传统通过内存拷贝的方式将几个小 Buffer 合并成一个大的 Buffer。
* Netty 的文件传输采用了 transferTo 方法，它可以直接将文件缓冲区的数据发送到目标 Channel，避免了传统通过循环 write 方式导致的内存拷贝问题。



## 8. netty和tomcat的区别

* 作用不同：Tomcat 是 Servlet 容器，可以视为 Web 服务器，而 Netty 是异步事件驱动的网络应用程序框架和工具用于简化网络编程，例如TCP和UDP套接字服务器。
* 协议不同：Tomcat 是基于 http 协议的 Web 服务器，而 Netty 能通过编程自定义各种协议，因为 Netty 本身自己能编码/解码字节流，所以 Netty 可以实现，HTTP 服务器、FTP 服务器、UDP 服务器、RPC 服务器、WebSocket 服务器、Redis 的 Proxy 服务器、MySQL 的 Proxy 服务器等等。